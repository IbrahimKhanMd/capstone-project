{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonar Mind : Echoes of Knowledge ,Made Clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Composition \n",
    "\n",
    "1. 22BCB7267 Patan Mohammed Ibrahim Khan\n",
    "2. 22BCB7275 Duggireddy Venu\n",
    "3. 22BCB7132 Maile Hruday Raj\n",
    "4. 22BCB7003 Sanikommu Divakar Reddy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Educational environments today face significant challenges in accommodating diverse learning needs, especially for students with language barriers, attention difficulties, or hearing impairments. Many students struggle to keep up with live lectures and often find themselves excluded from meaningful classroom participation. To address these pressing concerns, we developed Sonar Mind: The Educational Assistant, a comprehensive AI-powered tool specifically designed to make classroom content accessible and engaging for all learners. Our system transforms traditional lecture audio into interactive, searchable formats through the integration of Generative AI, Retrieval-Augmented Generation technology, and a user-friendly Gradio interface with an appealing aesthetic.\n",
    "\n",
    "We built this platform around the core understanding that modern classrooms require flexible, adaptive solutions that meet students where they are in their learning journey. Our implementation includes essential AI capabilities such as intelligent question-answering systems, accurate speech-to-text transcription, sophisticated semantic search functionality, and personalized quiz generation features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives:\n",
    "1. Bridge Educational Accessibility Gaps\n",
    "   1. Eradicate barriers faced by deaf and hard-of-hearing students in traditional lecture-based learning environments\n",
    "   2. Provide real-time, automated transcription services to replace dependency on manual interpreters and note-takers\n",
    "   3. Guarantee equal access to educational content regardless of physical abilities or learning differences\n",
    "2. Transform Passive Learning into Interactive Education\n",
    "    1. Convert static lecture audio into searchable, interactive digital resources\n",
    "\t2. Enable students to actively engage with course material through AI-powered question-answering capabilities\n",
    "\t3. Create engaging learning experiences that adapt to individual student needs and learning paces\n",
    "3. Enhance Learning Retention and Self-Assessment\n",
    "\t1. Implement automated quiz generation based on lecture content to reinforce understanding\n",
    "\t2. Provide feedback and contextual answers derived from actual classroom discussions\n",
    "\t3. Support self-paced learning by allowing students to revisit and review specific lecture segments\n",
    "4. Enhance Inclusive and Equitable Educational Environments\n",
    "     1. Reduce achievement gaps between students with disabilities and their peers\n",
    "\t 2. Create universally beneficial learning tools that enhance the educational experience for all students\n",
    "\t 3. Promote educational equity by ensuring that learning barriers do not limit student potential or participation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen AI functionalities:\n",
    " \n",
    "1) Audio understanding: Uses Gemini's Speech-to-Text models to translate spoken lectures into text.\n",
    "2) Structured output/JSON mode/controlled generation: For consistent rendering, quiz answers and content adhere to rigorous structured formats.\n",
    "3) Few-shot prompting: Assists Gemini in producing precise responses, even in cases where transcripts contain small transcription errors.\n",
    "4) Grounding: Ensures that no information from outside sources or hallucinations are used in any AI-generated responses, where all answers are based exclusively on the uploaded lecture.\n",
    "5) Embeddings & vector databases: For effective vector-based retrieval, lecture text is converted into semantic embeddings and saved in Chroma DB.\n",
    "6) Retrieval-Augmented Generation (RAG): Combines Gemini Flash's generation capabilities with vector retrieval to provide    accurate and contextual responses\n",
    "7) Vector search: To find the most pertinent portions of the lecture, natural language queries initiate vector similarity lookups.\n",
    "8) Function calling (Lang Chain): Organizes evaluators, chains, as well as retrievers into a seamless and modular pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Workflow\n",
    "The assistant's architecture follows a simple flow as follows:\n",
    "\n",
    "1) Audio Upload\n",
    "\n",
    "Teachers or students can upload recorded lecture audio using Gradio's user-friendly interface. Gemini STT transforms the audio into accurate text transcripts.\n",
    "\n",
    "2) Transcription and Chunking\n",
    "\n",
    "LangChain's RecursiveCharacterTextSplitter divides the raw transcript into digestible portions for efficient search and retrieval.\n",
    "\n",
    "3) Embedding & Indexing\n",
    "\n",
    "Each chunk is embedded using Gemini's embedding models and stored in ChromaDB for reliable and fast vector-based search.\n",
    "\n",
    "4) Lecture Content Querying (RAG)\n",
    "\n",
    "Students use natural language when typing questions. While the assistant uses ChromaDB for semantic search, then Gemini Flash generates grounded and context-aware responses using the lecture fragments that were retrieved.\n",
    "\n",
    "5) Quiz Generation\n",
    "\n",
    "Few-shot prompting is used to automatically generate quizzes from the lecture transcript. It is easy to render outputs into interactive UI elements because they follow a JSON schema.\n",
    "\n",
    "6) Interactive Feedback Mechanism\n",
    "\n",
    "Students evaluate themselves through gamified,quizzes with streak tracking and real-time scoring, which makes learning fun and inclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Stack Used \n",
    "\n",
    "Throughout this project, there were a few key tech stacks used to create a seamless workflow.\n",
    "\n",
    "1. **Gemini Flash / Pro**: For transcription, text generation, embeddings, and quiz creation.\n",
    "\n",
    "\n",
    "\n",
    "2. **ChromaDB**: A vector store that facilitates quick semantic searches.\n",
    "\n",
    "\n",
    "3. **LangChain**: Oversees the coordination of model prompting, retrieval, as well as function calls.\n",
    "\n",
    "\n",
    "4. **Gradio**: A simplified user interface for audio upload, question interaction, as well as quiz feedback.\n",
    "\n",
    "\n",
    "5. **Python**: Serves as the foundation for connecting all services and workflows.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries & Packages\n",
    "In the code cell below, some important libraries and packages are installed to be able to run this notebook.  These include tools to access Google’s Generative AI models, creating interactive user interfaces with Gradio, as well as building smart applications with LangChain. On the other hand, ChromaDB is used to store and search text efficiently, whereas FastAPI is included to enable deployment of the project as a web service if needed. By installing these libraries and packages, it will ensure that all components of the AI assistant work properly within the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 74136,
     "status": "ok",
     "timestamp": 1744766484753,
     "user": {
      "displayName": "Jun Loh",
      "userId": "12031303531432355173"
     },
     "user_tz": -480
    },
    "id": "LXxWKQ9ZoyYK",
    "outputId": "ab2104b9-0039-414f-b80e-9200e821e656",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai\n",
    "!pip install langchain_community\n",
    "!pip install gradio==4.14.0 google-generativeai==0.3.2 --quiet\n",
    "!pip install -qU langchain-google-genai\n",
    "!pip install chromadb\n",
    "!pip install fastapi==0.112.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3958,
     "status": "error",
     "timestamp": 1744766488726,
     "user": {
      "displayName": "Jun Loh",
      "userId": "12031303531432355173"
     },
     "user_tz": -480
    },
    "id": "t2O0chT9p0se",
    "outputId": "5937b1a7-1b15-4158-c6d9-d3d7e8de08ac",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "import gradio as gr\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "from google.api_core.exceptions import GoogleAPIError, NotFound, PermissionDenied\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.colab import userdata\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Audio Dataset\n",
    "In order to have a thorough testing using this Inclusive Classroom Assistant, there are 4 different types of audio in the dataset path as below, such as:\n",
    "\n",
    "\n",
    "1) Short audio with clean background sound\n",
    "\n",
    "\n",
    "2) Medium-length long audio with moderate noise\n",
    "\n",
    "\n",
    "3) Long audio with clean background sound\n",
    "\n",
    "\n",
    "4) Long audio with heavy background noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/kaggle/input/kaggle-capstone-project-audio-library\"\n",
    "print(\"Files:\", os.listdir(dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Walkthrough on Each Function\n",
    "The project starts by allowing users to upload audio files, which are then converted into text by Gemini's speech-to-text model. This transcript can be formatted, stored, and updated by users as needed. The transcript is then segmented into smaller parts and stored in a vector database using ChromaDB to enable efficient semantic search. This makes it possible for the Retrieval-Augmented Generation (RAG) chain to precisely respond to user inquiries using just the transcript. The system also has a quiz generation feature that creates multiple-choice questions from the transcript in structured JSON format using few-shot prompting to facilitate quiz-based learning. Interactive elements let users choose their responses, receive immediate feedback, and monitor their scores. Besides, the user-friendly Gradio interface combines these features to provide students a practical and interesting tool. Some of the GenAI features used throughout the project to guarantee the assistant produces relevant, context-aware outputs depending on lecture material are grounding, audio understanding, embeddings, vector search, structured output, and few-shot prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Global State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global State Management\n",
    "These variables store shared state across the notebook. They act like session memory that enables multiple parts of the app (transcription, RAG, quiz, etc.) to access and update the same data.\n",
    "\n",
    "| Variable               | Purpose                                                       |\n",
    "|------------------------|---------------------------------------------------------------|\n",
    "| full_transcript        | Holds all transcribed chunks from audio                       |\n",
    "| vector_db              | Stores the Chroma vector database for vector search           |\n",
    "| rag_chain              | Holds the Retrieval-Augmented Generation pipeline             |\n",
    "| current_correct_answer | Tracks quiz answers (used in quiz evaluation logic)           |\n",
    "\n",
    "\n",
    "These variables ensure seamless interaction between various functions, as well as to help in maintaining the assistant's overall state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 78202,
     "status": "aborted",
     "timestamp": 1744766488635,
     "user": {
      "displayName": "Jun Loh",
      "userId": "12031303531432355173"
     },
     "user_tz": -480
    },
    "id": "LYS6vjQZqCKT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------- Global State -------------\n",
    "full_transcript = []\n",
    "lecture_index = []\n",
    "vector_db = None\n",
    "rag_chain = None\n",
    "current_correct_answer = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription Module (Gemini STT)\n",
    "This section will explain on uploading audio to Gemini API, generating transcriptions using a controlled prompt, storing and resetting the full transcript state, and integrating with Gradio (upload, clear, display). Besides, it will also demonstrates on the audio understanding, function calling, and grounded prompting (plain text, no hallucinations).\n",
    "\n",
    "\n",
    "The definition of each function will be defined as below:\n",
    "\n",
    "* **get_full_transcript_text()**: Combines all the transcribed pieces into one readable paragraph.\n",
    "\n",
    "* **clear_transcript_data()**: Resets the transcript list, allowing a fresh start when a new audio file is uploaded.\n",
    "\n",
    "On the other hand, the other function handles the speech-to-text transcription. When a user uploads an audio file, **transcribe_audio_chunk()** sends the file to Google’s Gemini AI. Then, it will listen to the audio, turns it into text, and returns it. However, if anything goes wrong such as a permission issue or upload failure, the function will catch it and display a helpful error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the Output\n",
    "Moreover, **format_transcription_result()** simply returns the transcript as it is. This function is kept separate in case formatting or cleaning is needed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Transcription Handler\n",
    "The functions named handle_transcription_request() as well as handle_clear_transcript() manage the interaction between the user and the app for transcription.\n",
    "\n",
    "* **handle_transcription_request()**: This function is triggered when a user uploads an audio file. It sends the audio for transcription, stores the result, and updates the transcript display.\n",
    "\n",
    "* **handle_clear_transcript()**: Resets the transcript area in the app when the user clicks the \"Clear Full Transcript\" button.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking, Embedding, Vector DB & RAG Setup\n",
    "This section will explain on chunking of transcript into semantically rich and overlapping segments, Gemini embedding and storage in Chroma DB, LangChain-powered RAG pipeline using Gemini for Q&A, as well as few-shot prompt examples for more grounded answers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This is where the transcript is prepared for smart search and question-answering.\n",
    "\n",
    "* **chunk_transcript()**: Breaks long text into smaller overlapping pieces.\n",
    "\n",
    "* **create_vector_db()**: Turns those chunks into vector form using embeddings and stores them in a searchable database called Chroma.\n",
    "\n",
    "* **setup_rag_chain()**: Connects the Chroma database to a Gemini-powered answering system. It ensures that when a user asks a question, the app can find the most relevant transcript pieces and provide a focused answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing the Transcript\n",
    "The **handle_indexing_request()** function brings everything together. It chunks the transcript, creates the searchable vector database, and sets up the RAG (Retrieval-Augmented Generation) chain. It must be run before users can start asking questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering (RAG)\n",
    "These below functions handle questions submitted by users.\n",
    "\n",
    "* **query()**: Sends a user question to the RAG system and retrieves an answer.\n",
    "\n",
    "* **answer_query_using_rag()**: Checks if the system is ready and then processes the user’s question based on the content of the transcript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz Generation with Few-Shot Prompting\n",
    "This section sets up an AI-powered quiz generator. The function named **setup_quiz_chain()** gives the model a structured example and clear format. Hence, it creates 5 multiple-choice questions using only the transcript as reference. It returns the output in clean JSON format so that it is easy to display in the interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Quiz State\n",
    "**quiz_state** is used to keep track of the quiz progress. This includes the current question, score, as well as streak of correct answers. It acts like the memory for the quiz system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Quiz Questions\n",
    "This module uses Gemini to generate multiple-choice questions from transcript data. It has a structured JSON output for clean parsing, uses few-shot prompting to improve clarity, and controls generation using LangChain JsonOutputParser. Besides, it also has a full quiz lifecycle from generation, to answer checking, and last to navigation. Lastly, it provides scoring and streak logic for a fun and interactive learning experience. Regarding the output, it is displayed in Gradio with buttons, feedback, and scoring badges.\n",
    "\n",
    "The function named **generate_quiz()** calls the quiz chain to generate new questions based on the transcript. It prepares the quiz state and shows the first question along with 4 answer options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz Answer Evaluation\n",
    "Furthermore, the function called **select_answer()** checks if the selected option is correct. It updates the score and tracks the user’s answer streak. Besides, it also prevents the same question from being answered more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Question Handler\n",
    "For **advance_to_next_question()** function, it helps to move to the next quiz question if the user has already answered the current one. When the quiz is done, it calculates the final score as a percentage and shows the result in color-coded text (green if you pass, red if not).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz and UI Helper Functions\n",
    "* **generate_quiz_and_buttons()**: Generates a quiz and sets up the option buttons in the UI.\n",
    "\n",
    "* **select_answer_and_update()**: Checks the selected answer and updates the quiz feedback.\n",
    "\n",
    "* **load_transcript()**: Returns the current transcript only so that it can be expanded for more use later.\n",
    "\n",
    "* **clear_transcript()**: Resets the transcript display area.\n",
    "\n",
    "* **handle_query_request()**: Handles the user’s question submission and returns an answer based on the indexed transcript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 78202,
     "status": "aborted",
     "timestamp": 1744766488636,
     "user": {
      "displayName": "Jun Loh",
      "userId": "12031303531432355173"
     },
     "user_tz": -480
    },
    "id": "5DR2hUwcqMwx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------- Transcript State Management -------------\n",
    "def get_full_transcript_text():\n",
    "    return \" \".join(full_transcript)\n",
    "\n",
    "def clear_transcript_data():\n",
    "    global full_transcript\n",
    "    full_transcript = []\n",
    "    print(\"Transcript data cleared.\")\n",
    "    return \"\"\n",
    "\n",
    "# ------------- Gemini STT Function -------------\n",
    "def transcribe_audio_chunk(audio_path):\n",
    "    try:\n",
    "        client = genai.Client(api_key=API_KEY)\n",
    "        uploaded_file = client.files.upload(file=audio_path)\n",
    "        print(f\"✅ File uploaded. URI: {uploaded_file.uri}, Name: {uploaded_file.name}\")\n",
    "\n",
    "        prompt = (\n",
    "            \"Please perform speech-to-text transcription for the provided audio file. \"\n",
    "            \"Output the transcribed text followed by the key points as a numbered list. \"\n",
    "            \"Do not use any JSON formatting—just return plain text.\"\n",
    "        )\n",
    "\n",
    "        print(\"🚀 Sending transcription request to Gemini...\")\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=[prompt, uploaded_file]\n",
    "        )\n",
    "\n",
    "        if not response.candidates:\n",
    "            block_reason = response.prompt_feedback.block_reason if response.prompt_feedback else \"Unknown\"\n",
    "            return f\"Transcription failed. Block Reason: {block_reason}\"\n",
    "\n",
    "        candidate = response.candidates[0]\n",
    "        if hasattr(candidate.content, 'parts') and candidate.content.parts:\n",
    "            transcript = candidate.content.parts[0].text\n",
    "            print(\"✅ Transcription successful.\")\n",
    "            return transcript\n",
    "        else:\n",
    "            return \"Error: Failed to parse transcription response.\"\n",
    "\n",
    "    except PermissionDenied as e:\n",
    "        return f\"❌ Permission Denied: {e.message}\"\n",
    "    except NotFound as e:\n",
    "        return f\"❌ Resource Not Found: {e.message}\"\n",
    "    except GoogleAPIError as e:\n",
    "        return f\"❌ API Error: {e.message}\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Unexpected Error: {str(e)}\"\n",
    "\n",
    "# ------------- Formatting the Output -------------\n",
    "def format_transcription_result(result_text):\n",
    "    return result_text\n",
    "\n",
    "# ------------- Gradio Transcription Handler -------------\n",
    "def handle_transcription_request(audio_file):\n",
    "    if audio_file is None:\n",
    "        return \"\", get_full_transcript_text(), gr.update(value=None), \"Transcription not initiated.\", \"Input declined. No audio file provided.\"\n",
    "\n",
    "    transcript_text = transcribe_audio_chunk(audio_file)\n",
    "    formatted_chunk = format_transcription_result(transcript_text)\n",
    "    full_transcript.append(formatted_chunk)\n",
    "\n",
    "    return (\n",
    "        formatted_chunk,\n",
    "        get_full_transcript_text(),\n",
    "        gr.update(value=None),\n",
    "        \"Transcription successful.\",\n",
    "        \"Input accepted. Audio file is being processed.\"\n",
    "    )\n",
    "\n",
    "def handle_clear_transcript():\n",
    "    clear_transcript_data()\n",
    "    return \"\", \"Transcript cleared.\"\n",
    "\n",
    "\n",
    "\n",
    "# ------------- Chunking, Embedding, Vector DB & RAG -------------\n",
    "\n",
    "def chunk_transcript(text, chunk_size: int = 800, overlap_size: int = 150):\n",
    "    # Optionally, you could call: text = correct_transcript_errors(text)\n",
    "    document = [Document(page_content=text)]\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap_size\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents=document)\n",
    "    print(f\"File split into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "# ------------- Chunking, Embedding, Vector DB & RAG -------------\n",
    "\n",
    "import asyncio\n",
    "\n",
    "def create_vector_db(text_chunks, collection_name=\"transcription-rag\"):\n",
    "    global vector_db\n",
    "    try:\n",
    "        # Ensure we have an event loop in the current thread\n",
    "        try:\n",
    "            asyncio.get_running_loop()\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            google_api_key=API_KEY\n",
    "        )\n",
    "\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=text_chunks,\n",
    "            embedding=embeddings,\n",
    "            collection_name=collection_name,\n",
    "            persist_directory=\"/content/chroma_db\"  # Ephemeral persist directory\n",
    "        )\n",
    "        print(f\"✅ Vector DB created with collection_name: {collection_name}\")\n",
    "        return vector_db\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error creating vector DB: {str(e)}\")\n",
    "\n",
    "\n",
    "def setup_rag_chain(vector_db):\n",
    "    if not vector_db:\n",
    "        raise ValueError(\"Vector DB not initialized!\")\n",
    "\n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            google_api_key=API_KEY\n",
    "        )\n",
    "\n",
    "        # Few-shot query rewriting prompt\n",
    "        query_prompt = PromptTemplate.from_template(\"\"\"\n",
    "            You are an AI assistant that helps rephrase queries.\n",
    "\n",
    "            Example 1:\n",
    "            Original Question: Who is Master Sito?\n",
    "            Alternative Queries:\n",
    "              1. According to the transcript, what is Master Sito's role?\n",
    "              2. What does the transcript state about Master Sito?\n",
    "              3. How is Master Sito described in the lecture?\n",
    "\n",
    "            Example 2:\n",
    "            Original Question: Who is Master Sito?\n",
    "            Even if the transcript contains a minor typo (e.g., 'Master Ceto'),\n",
    "            assume the intended name is Master Sito.\n",
    "\n",
    "            Now, given the original question: {question}\n",
    "            Generate three alternative queries:\n",
    "        \"\"\")\n",
    "\n",
    "        retriever = MultiQueryRetriever.from_llm(\n",
    "            retriever=vector_db.as_retriever(search_kwargs={\"k\": 4}),\n",
    "            llm=llm,\n",
    "            prompt=query_prompt\n",
    "        )\n",
    "\n",
    "        # Main prompt for answering with grounding and few-shot examples\n",
    "        main_template = \"\"\"\n",
    "            You are an educational assistant. Answer the user's question based solely on the transcript context provided.\n",
    "            Disregard minor transcription errors (for example, if the transcript has \"Master Ceto\" but context indicates it should be \"Master Sito\").\n",
    "            If the answer is explicitly stated, provide it exactly. Otherwise, reply \"I don’t know.\"\n",
    "\n",
    "            Few-shot examples:\n",
    "            ---------------------\n",
    "            Transcript Example 1:\n",
    "            \"Master Sito said: 'Face life with humor.'\"\n",
    "            Q: What did Master Sito say about life?\n",
    "            A: Face life with humor.\n",
    "            ---------------------\n",
    "            Transcript Example 2:\n",
    "            \"According to the lecture, Master Sito is a monk living in seclusion.\"\n",
    "            Q: Who is Master Sito?\n",
    "            A: He is a monk.\n",
    "            ---------------------\n",
    "            Now, using the transcript below:\n",
    "            Transcript:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(template=main_template)\n",
    "\n",
    "        chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        print(\"RAG chain setup complete!\")\n",
    "        return chain\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error setting up RAG chain: {str(e)}\")\n",
    "\n",
    "def handle_indexing_request(transcript_text):\n",
    "    global vector_db, rag_chain\n",
    "    if not transcript_text or len(transcript_text.strip()) == 0:\n",
    "        return \"⚠️ Transcript is empty. Please transcribe or paste something first.\"\n",
    "    try:\n",
    "        chunks = chunk_transcript(transcript_text)\n",
    "        vector_db = create_vector_db(chunks)\n",
    "        rag_chain = setup_rag_chain(vector_db)\n",
    "        return f\"✅ Indexing complete. {len(chunks)} chunks indexed.\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Indexing failed: {str(e)}\"\n",
    "\n",
    "def query(chain, question: str):\n",
    "    if not chain:\n",
    "        print(\"RAG chain not initialized!\")\n",
    "    try:\n",
    "        return chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error processing query: {str(e)}\")\n",
    "\n",
    "def answer_query_using_rag(user_query):\n",
    "    global rag_chain\n",
    "    if not rag_chain:\n",
    "        return \"⚠️ Please index the transcript first.\"\n",
    "    try:\n",
    "        result = query(rag_chain, user_query)\n",
    "        return f\"💬 {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error: {str(e)}\"\n",
    "\n",
    "#-------------Quiz Generation with few shot prompting---------------------------------------------\n",
    "def setup_quiz_chain():\n",
    "    try:\n",
    "        llm_quiz = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            temperature=0.1,\n",
    "            # Consider setting a reasonable max_tokens limit, e.g., max_tokens=1024\n",
    "            max_tokens=None,\n",
    "            # Consider setting an explicit timeout, e.g., timeout=120\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            google_api_key=API_KEY\n",
    "        )\n",
    "\n",
    "        quiz_template = \"\"\"\n",
    "            You are an educational assistant. Your task is to generate 5 multiple-choice quiz questions based only on the transcript provided below.\n",
    "            Please return the output strictly as valid JSON. Do not include any introductory text or markdown formatting around the JSON object.\n",
    "            The JSON should be a list containing 5 objects, each following this format:\n",
    "\n",
    "            {{\n",
    "              \"question\": \"Your quiz question here.\",\n",
    "              \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
    "              \"answer\": \"The correct option (must exactly match one of the options)\"\n",
    "            }}\n",
    "\n",
    "            Transcript:\n",
    "            {transcript}\n",
    "\n",
    "            JSON Output:\n",
    "        \"\"\" # Added \"JSON Output:\" hint and refined instructions slightly\n",
    "\n",
    "        quiz_prompt = PromptTemplate.from_template(quiz_template)\n",
    "        # For standard JSON:\n",
    "        parser = JsonOutputParser()\n",
    "\n",
    "        # Update the chain to use the JsonOutputParser\n",
    "        chain = (\n",
    "            {\"transcript\": RunnablePassthrough()}\n",
    "            | quiz_prompt\n",
    "            | llm_quiz\n",
    "            | parser # <-- Use JsonOutputParser instead of StrOutputParser\n",
    "        )\n",
    "        print(\"Quiz chain setup complete!\")\n",
    "        return chain\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error setting up Quiz chain: {str(e)}\")\n",
    "\n",
    "\n",
    "# --- Global Quiz State ---\n",
    "quiz_state = None\n",
    "\n",
    "# --- Function to Generate Quiz ---\n",
    "def generate_quiz(transcript: str):\n",
    "    global quiz_state\n",
    "    if not transcript or transcript.strip() == \"\":\n",
    "        return \"⚠️ Please provide a transcript.\", [], \"No quiz generated.\"\n",
    "    try:\n",
    "        chain = setup_quiz_chain()\n",
    "        output = chain.invoke({\"transcript\": transcript})\n",
    "        print(\"DEBUG - Chain output:\", output)\n",
    "        quiz_data = output  # Already parsed JSON from JsonOutputParser.\n",
    "    except Exception as e:\n",
    "        return f\"Quiz generation failed: {str(e)}\", [], \"Error occurred.\"\n",
    "    if not quiz_data or len(quiz_data) == 0:\n",
    "        return \"⚠️ No quiz questions returned by the model.\", [], \"\"\n",
    "\n",
    "    # Initialize quiz state with an additional 'answered' flag.\n",
    "    quiz_state = {\n",
    "    \"questions\": quiz_data,\n",
    "    \"current_index\": 0,\n",
    "    \"score\": 0,\n",
    "    \"streak\": 0,            # New: Track consecutive correct answers.\n",
    "    \"answered\": False       # New: Flag to indicate if the current question is answered.\n",
    "}\n",
    "\n",
    "    first_question = quiz_data[0]\n",
    "    return first_question[\"question\"], first_question[\"options\"], \"\"\n",
    "\n",
    "# --- Function to Evaluate Answer (without advancing to next question) ---\n",
    "def select_answer(index: int):\n",
    "    global quiz_state\n",
    "    if not quiz_state or \"questions\" not in quiz_state:\n",
    "        return \"No quiz generated. Please generate a quiz first.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"⚠️\", \"Score: 0 | Streak: 0\"\n",
    "\n",
    "    # Prevent re-answering if the question was already answered.\n",
    "    if quiz_state.get(\"answered\", False):\n",
    "        current_question = quiz_state[\"questions\"][quiz_state[\"current_index\"]]\n",
    "        options = current_question.get(\"options\", [])\n",
    "        btn_labels = [options[i] if i < len(options) else \"N/A\" for i in range(4)]\n",
    "        return (current_question[\"question\"], btn_labels[0], btn_labels[1], btn_labels[2], btn_labels[3],\n",
    "                \"You have already answered. Click 'Next Question' to continue.\",\n",
    "                f\"Score: {quiz_state.get('score', 0)} | Streak: {quiz_state.get('streak', 0)}\")\n",
    "\n",
    "    current_question = quiz_state[\"questions\"][quiz_state[\"current_index\"]]\n",
    "    options = current_question.get(\"options\", [])\n",
    "    if index >= len(options):\n",
    "        return \"Invalid option selected.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"Error\", f\"Score: {quiz_state.get('score', 0)} | Streak: {quiz_state.get('streak', 0)}\"\n",
    "\n",
    "    selected_option = options[index]\n",
    "\n",
    "    # Check answer and update score and streak.\n",
    "    if selected_option == current_question[\"answer\"]:\n",
    "        feedback = \"Correct!\"\n",
    "        quiz_state[\"score\"] += 1\n",
    "        quiz_state[\"streak\"] += 1\n",
    "    else:\n",
    "        feedback = f\"Incorrect. The correct answer was: {current_question['answer']}.\"\n",
    "        quiz_state[\"streak\"] = 0\n",
    "\n",
    "    quiz_state[\"answered\"] = True  # Mark the question as answered.\n",
    "    btn_labels = [options[i] if i < len(options) else \"N/A\" for i in range(4)]\n",
    "    score_text = f\"Score: {quiz_state['score']} | Streak: {quiz_state['streak']}\"\n",
    "    return (current_question[\"question\"], btn_labels[0], btn_labels[1], btn_labels[2], btn_labels[3],\n",
    "            feedback, score_text)\n",
    "\n",
    "# --- Function to Advance to the Next Question ---\n",
    "def advance_to_next_question():\n",
    "    global quiz_state\n",
    "    if not quiz_state or \"questions\" not in quiz_state:\n",
    "        return \"No quiz generated. Please generate a quiz first.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"⚠️\", \"Score: 0 | Streak: 0\"\n",
    "\n",
    "    if not quiz_state.get(\"answered\", False):\n",
    "        return \"Please select an answer before proceeding.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"⚠️\", f\"Score: {quiz_state['score']} | Streak: {quiz_state['streak']}\"\n",
    "\n",
    "    quiz_state[\"current_index\"] += 1\n",
    "    quiz_state[\"answered\"] = False  # Reset the answered flag.\n",
    "    if quiz_state[\"current_index\"] < len(quiz_state[\"questions\"]):\n",
    "        next_q = quiz_state[\"questions\"][quiz_state[\"current_index\"]]\n",
    "        options = next_q.get(\"options\", [])\n",
    "        btn_labels = [options[i] if i < len(options) else \"N/A\" for i in range(4)]\n",
    "        return (next_q[\"question\"], btn_labels[0], btn_labels[1], btn_labels[2], btn_labels[3],\n",
    "                \"\", f\"Score: {quiz_state['score']} | Streak: {quiz_state['streak']}\")\n",
    "    else:\n",
    "        score = quiz_state[\"score\"]\n",
    "        total = len(quiz_state[\"questions\"])\n",
    "        percentage = round((score / total) * 100)\n",
    "        color = \"red\" if percentage < 60 else \"green\"\n",
    "        # Display final score with some HTML styling.\n",
    "        percent_display = f\"<span style='color:{color}; font-weight:bold;'>{percentage}%</span>\"\n",
    "        final_msg = f\"Quiz complete! Your final score is {score} out of {total}: {percent_display}.\"\n",
    "        quiz_state = None\n",
    "        return final_msg, \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "\n",
    "# --- Combined function to update quiz question & button labels on generation ---\n",
    "def generate_quiz_and_buttons(transcript: str):\n",
    "    question, options, feedback = generate_quiz(transcript)\n",
    "    btn_labels = [\"N/A\", \"N/A\", \"N/A\", \"N/A\"]\n",
    "    if isinstance(options, list):\n",
    "        for i in range(min(len(options), 4)):\n",
    "            btn_labels[i] = options[i]\n",
    "    score_text = \"Score: 0 | Streak: 0\"\n",
    "    return question, btn_labels[0], btn_labels[1], btn_labels[2], btn_labels[3], feedback, score_text\n",
    "\n",
    "def select_answer_and_update(index: int):\n",
    "    # (Call our select_answer function.)\n",
    "    return select_answer(index)\n",
    "\n",
    "def load_transcript(full_text):\n",
    "    # For now, simply return the same text.\n",
    "    # Adjust this function based on your intended behavior.\n",
    "    return full_text\n",
    "\n",
    "def clear_transcript():\n",
    "    # This dummy implementation clears the transcript and returns a cleared status message.\n",
    "    return \"\", \"Transcript cleared.\"\n",
    "\n",
    "def handle_query_request(user_query):\n",
    "    if not user_query or not user_query.strip():\n",
    "        return \"⚠️ Please enter a valid question about the lecture.\"\n",
    "\n",
    "    # Hypothetical function that uses your indexed transcript + LLM:\n",
    "    return answer_query_using_rag(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 78249,
     "status": "aborted",
     "timestamp": 1744766488686,
     "user": {
      "displayName": "Jun Loh",
      "userId": "12031303531432355173"
     },
     "user_tz": -480
    },
    "id": "sQrBXrVkqe_z",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Gradio Interface with Custom Golden Theme ------------------\n",
    "\n",
    "with gr.Blocks(\n",
    "    theme=\"d8ahazard/material_design_rd\",\n",
    "    css=\"\"\"\n",
    "    @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@600;700&family=Raleway:wght@400;500&display=swap');\n",
    "\n",
    "    /* Universal font styles with Golden Theme */\n",
    "    body,\n",
    "    .gradio-container,\n",
    "    .gr-button,\n",
    "    .gr-markdown,\n",
    "    .gr-textbox,\n",
    "    h1, h2, h3, h4, p {\n",
    "      font-family: 'Raleway', sans-serif !important;\n",
    "      color: #f5deb3 !important;  /* warm golden beige */\n",
    "      letter-spacing: 0.05em;\n",
    "      line-height: 1.6;\n",
    "      background-color: #1a1a1a !important; /* deep dark background */\n",
    "      margin: 0;\n",
    "      padding: 0;\n",
    "    }\n",
    "\n",
    "    /* Accent color definition for buttons and highlights */\n",
    "    .accent-bg {\n",
    "      background: linear-gradient(135deg, #caa44d, #e6c85d) !important; /* gold gradient */\n",
    "      font-family: 'Playfair Display', serif !important;\n",
    "      color: #1a1a1a !important;  /* dark text */\n",
    "      font-weight: 700;\n",
    "      border-radius: 6px;\n",
    "      box-shadow: 0 0 8px rgba(202,164,77,0.7);\n",
    "      transition: all 0.3s ease-in-out;\n",
    "    }\n",
    "    .accent-bg:hover {\n",
    "      background: linear-gradient(135deg, #e6c85d, #f5e08a) !important;\n",
    "      box-shadow: 0 0 12px rgba(255,215,0,0.9);\n",
    "    }\n",
    "\n",
    "    /* Elegant heading style */\n",
    "    .golden-heading {\n",
    "      font-family: 'Playfair Display', serif !important;\n",
    "      letter-spacing: 0.1em;\n",
    "      font-size: 30px;\n",
    "      color: #ffd700 !important;\n",
    "      text-shadow: 0 0 6px rgba(255,215,0,0.8);\n",
    "    }\n",
    "\n",
    "    /* Smooth typewriter effect for header */\n",
    "    .typewriter {\n",
    "      overflow: hidden;\n",
    "      border-right: .15em solid #ffd700;\n",
    "      white-space: nowrap;\n",
    "      animation: typing 2.5s steps(30, end), blink-caret 0.75s step-end infinite;\n",
    "      width: fit-content;\n",
    "      font-weight: 700;\n",
    "      line-height: 1.8;\n",
    "    }\n",
    "    @keyframes typing {\n",
    "      from { width: 0; }\n",
    "      to { width: 100%; }\n",
    "    }\n",
    "    @keyframes blink-caret {\n",
    "      from, to { border-color: transparent; }\n",
    "      50% { border-color: #ffd700; }\n",
    "    }\n",
    "\n",
    "    /* Logo bounce animation */\n",
    "    #bot-logo img {\n",
    "      animation: bounce 1.2s ease infinite !important;\n",
    "      border-radius: 8px;\n",
    "      width: 90px;\n",
    "      height: 90px;\n",
    "      object-fit: contain;\n",
    "      margin-right: 8px;\n",
    "    }\n",
    "    @keyframes bounce {\n",
    "      0%, 100% { transform: translateY(0); }\n",
    "      50% { transform: translateY(-8px); }\n",
    "    }\n",
    "\n",
    "    /* Dark gold textboxes */\n",
    "    .gr-textbox, .gr-textbox textarea, .gr-textbox input {\n",
    "      background-color: #262626 !important;\n",
    "      color: #f5deb3 !important;\n",
    "      border: 1px solid #caa44d !important;\n",
    "      border-radius: 6px;\n",
    "      padding: 4px 8px;\n",
    "    }\n",
    "    ::placeholder {\n",
    "      color: #d4af37 !important;\n",
    "      opacity: 0.8;\n",
    "    }\n",
    "\n",
    "    /* Header spacing */\n",
    "    #header {\n",
    "      margin-bottom: 16px;\n",
    "    }\n",
    "    .tab-content {\n",
    "      padding: 16px;\n",
    "    }\n",
    "\n",
    "    /* Gold scoreboard */\n",
    "    #quiz-scoreboard {\n",
    "      border: 2px solid #ffd700;\n",
    "      padding: 8px;\n",
    "      margin-bottom: 8px;\n",
    "      font-family: 'Playfair Display', serif;\n",
    "      color: #ffd700;\n",
    "      background-color: #1a1a1a;\n",
    "      text-align: right;\n",
    "    }\n",
    "\n",
    "    /* Golden panel for prompts/feedback */\n",
    "    .golden-panel {\n",
    "      border: 2px solid #caa44d;\n",
    "      background-color: #111;\n",
    "      padding: 8px;\n",
    "      margin-bottom: 8px;\n",
    "      font-family: 'Raleway', sans-serif;\n",
    "      color: #f5deb3 !important;\n",
    "      text-align: center;\n",
    "      text-shadow: 0 0 4px rgba(202,164,77,0.7);\n",
    "    }\n",
    "\n",
    "    /* Tab label styles */\n",
    "    .gradio-container .tabs button {\n",
    "      font-family: 'Playfair Display', serif !important;\n",
    "      color: #f5deb3 !important;\n",
    "      background-color: transparent !important;\n",
    "      border: none !important;\n",
    "    }\n",
    "    .gradio-container .tabs button:hover {\n",
    "      background-color: #333 !important;\n",
    "      color: #ffd700 !important;\n",
    "    }\n",
    "    .gradio-container .tabs button.selected {\n",
    "      color: #ffd700 !important;\n",
    "      text-shadow: 0 0 4px rgba(255,215,0,0.8);\n",
    "    }\n",
    "\n",
    "    /* Hide audio file icon */\n",
    "    .gradio-audio .file-drop svg {\n",
    "        display: none !important;\n",
    "    }\n",
    "    \"\"\"\n",
    ") as app:\n",
    "\n",
    "    gr.Markdown('<link href=\"https://fonts.googleapis.com/css2?family=Playfair+Display:wght@600;700&family=Raleway:wght@400;500&display=swap\" rel=\"stylesheet\">')\n",
    "\n",
    "    with gr.Row():\n",
    "         with gr.Column(scale=1):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                <h2 class=\"golden-heading typewriter\" style=\"margin: 0;\">\n",
    "                    Inclusive Classroom Assistant\n",
    "                </h2>\n",
    "                <p style=\"margin: 4px 0 0 0; font-size: 14px; color: #f5deb3;\">\n",
    "                    Upload audio, transcribe, index, and ask anything about your lecture!\n",
    "                </p>\n",
    "                \"\"\",\n",
    "                elem_id=\"header\"\n",
    "            )\n",
    "\n",
    "    # Tab 1\n",
    "    with gr.Tab(\"🎙️ Transcription & Indexing\") as tab1:\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"<h3 style='color:#ffd700;'>Transcription Input</h3>\")\n",
    "                audio_input = gr.Audio(type=\"filepath\", show_label=False)\n",
    "                transcribe_button = gr.Button(\"Transcribe Chunk\", elem_classes=\"accent-bg\")\n",
    "                transcription_input_status_textbox = gr.Textbox(label=\"Transcription Input Status\", lines=1, interactive=False)\n",
    "                latest_chunk_textbox = gr.Textbox(label=\"Latest Transcript Chunk\", lines=10, interactive=False)\n",
    "                status_textbox = gr.Textbox(label=\"Status\", lines=1, interactive=False)\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"<h3 style='color:#ffd700;'>Full Transcript & Indexing</h3>\")\n",
    "                full_transcript_textbox = gr.Textbox(label=\"Full Lecture Transcript\", lines=20, interactive=False)\n",
    "                with gr.Row():\n",
    "                    index_button = gr.Button(\"Index Transcript for Search\", elem_classes=\"accent-bg\")\n",
    "                    clear_button = gr.Button(\"Clear Full Transcript\", elem_classes=\"accent-bg\")\n",
    "                indexing_status_display = gr.Textbox(label=\"Indexing Status\", lines=2, interactive=False)\n",
    "\n",
    "    # Tab 2\n",
    "    with gr.Tab(\"💬 Query Lecture Content\") as tab2:\n",
    "        gr.Markdown(\"<h3 style='color:#ffd700;'>Ask a question about the lecture content</h3>\")\n",
    "        with gr.Row():\n",
    "            query_input_textbox = gr.Textbox(\n",
    "                label=\"Ask a question\",\n",
    "                placeholder=\"E.g., What lesson did Sam learn?\",\n",
    "                lines=2\n",
    "            )\n",
    "            ask_button = gr.Button(\"Ask Question\", elem_classes=\"accent-bg\")\n",
    "        answer_display = gr.Markdown(\n",
    "            \"💡 Answer will appear here...\",\n",
    "            elem_classes=\"query-answer-box golden-panel\"\n",
    "        )\n",
    "\n",
    "    # Tab 3\n",
    "    with gr.Tab(\"📝 Quiz Generator\") as tab3:\n",
    "        scoreboard = gr.Markdown(\"Score: 0 | Streak: 0\", elem_id=\"quiz-scoreboard\")\n",
    "        gr.Markdown(\"<h3 style='color:#ffd700;'>Generate Quiz from Transcript</h3>\")\n",
    "        gr.Markdown(\"<p class='golden-panel'>Click <strong>Generate Quiz</strong> to start. Answer each question and review your score and correct answer streak after each question.</p>\")\n",
    "        generate_btn = gr.Button(\"Generate Quiz\", elem_classes=\"accent-bg\")\n",
    "        quiz_question = gr.Markdown(\"Question will appear here\", elem_classes=\"golden-panel\")\n",
    "        with gr.Row():\n",
    "            option_button1 = gr.Button(\"Option 1\", elem_classes=\"accent-bg\")\n",
    "            option_button2 = gr.Button(\"Option 2\", elem_classes=\"accent-bg\")\n",
    "            option_button3 = gr.Button(\"Option 3\", elem_classes=\"accent-bg\")\n",
    "            option_button4 = gr.Button(\"Option 4\", elem_classes=\"accent-bg\")\n",
    "        feedback_box = gr.Textbox(label=\"Feedback\", interactive=False, elem_classes=\"golden-panel\")\n",
    "        next_btn = gr.Button(\"Next Question\", elem_classes=\"accent-bg\")\n",
    "\n",
    "    # Button Callbacks\n",
    "    transcribe_button.click(\n",
    "        fn=handle_transcription_request,\n",
    "        inputs=[audio_input],\n",
    "        outputs=[latest_chunk_textbox, full_transcript_textbox, audio_input, status_textbox, transcription_input_status_textbox]\n",
    "    )\n",
    "    index_button.click(\n",
    "        fn=handle_indexing_request,\n",
    "        inputs=[full_transcript_textbox],\n",
    "        outputs=[indexing_status_display]\n",
    "    )\n",
    "    clear_button.click(\n",
    "        fn=clear_transcript_data,\n",
    "        inputs=None,\n",
    "        outputs=[full_transcript_textbox, status_textbox]\n",
    "    )\n",
    "    ask_button.click(\n",
    "        fn=handle_query_request,\n",
    "        inputs=[query_input_textbox],\n",
    "        outputs=[answer_display]\n",
    "    )\n",
    "    generate_btn.click(\n",
    "        fn=generate_quiz_and_buttons,\n",
    "        inputs=[full_transcript_textbox],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "    option_button1.click(\n",
    "        fn=lambda: select_answer_and_update(0),\n",
    "        inputs=[],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "    option_button2.click(\n",
    "        fn=lambda: select_answer_and_update(1),\n",
    "        inputs=[],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "    option_button3.click(\n",
    "        fn=lambda: select_answer_and_update(2),\n",
    "        inputs=[],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "    option_button4.click(\n",
    "        fn=lambda: select_answer_and_update(3),\n",
    "        inputs=[],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "    next_btn.click(\n",
    "        fn=advance_to_next_question,\n",
    "        inputs=[],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "\n",
    "app.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "Although the Inclusive Classroom Assistant enhances accessibility and engagement, it still faces a few challenges. Firstly, **transcription accuracy** can be affected by factors such as audio clarity, accents, background noise, or even overlapping speakers. This will lead to errors in the speech-to-text process. Secondly, the system has **limited contextual awareness**. It generates answers solely based on the uploaded lecture content. This means that it lacks broader subject knowledge. Finally, there is a **user dependency on uploads**. Currently, the system relies on manual audio uploads. However, this could be automated in the future by using classroom microphones."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 97258,
     "sourceType": "competition"
    },
    {
     "datasetId": 7157702,
     "sourceId": 11428326,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7167704,
     "sourceId": 11442058,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
